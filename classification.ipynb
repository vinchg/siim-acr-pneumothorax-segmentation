{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as pd\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pydicom\n",
    "import scipy\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.mask_functions import *\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from scipy.sparse import load_npz\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras_applications.resnext import ResNeXt50\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, LeakyReLU, core, Dropout, BatchNormalization, Concatenate, Dense, Flatten, GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to single GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Data - Create Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './siim/dicom-images-train/*/*/*.dcm'\n",
    "mask_path = './siim/train-rle.csv'\n",
    "images_to_ram = True # Low or high memory usage\n",
    "img_height = 512\n",
    "img_width = 512\n",
    "n_channels = 3\n",
    "batch_size = 8\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_df(train_path=train_path, mask_path=mask_path, images_to_ram=False, mask_img=False):\n",
    "    rles_df = pd.read_csv(mask_path)\n",
    "    rles_df.columns = ['ImageId', 'EncodedPixels']\n",
    "        \n",
    "    metadata_list = []\n",
    "    train_fps = glob(train_path)\n",
    "\n",
    "    if images_to_ram is True:\n",
    "        img_dict = {}\n",
    "    for i, fp in tqdm_notebook(enumerate(train_fps)):\n",
    "        metadata = {}\n",
    "        dcm = pydicom.dcmread(fp)\n",
    "        metadata['ImageId'] = dcm.SOPInstanceUID\n",
    "        metadata['FilePath'] = fp\n",
    "        \n",
    "        encoded_pixels_list = rles_df[rles_df['ImageId']==dcm.SOPInstanceUID]['EncodedPixels'].values\n",
    "        pneumothorax = False\n",
    "        for encoded_pixels in encoded_pixels_list:\n",
    "            if encoded_pixels != ' -1':\n",
    "                pneumothorax = True\n",
    "        \n",
    "        metadata['Pneumothorax'] = pneumothorax\n",
    "        \n",
    "        if images_to_ram is True:\n",
    "            metadata['PixelArray'] = dcm.pixel_array\n",
    "            if mask_img is True:\n",
    "                if pneumothorax is True:\n",
    "                    metadata['EncodedPixels'] = load_npz('siim/mask/'+dcm.SOPInstanceUID+'.npz').todense().astype('uint8')\n",
    "                else:\n",
    "                    metadata['EncodedPixels'] = np.zeros((1024, 1024))\n",
    "                \n",
    "        metadata_list.append(metadata)\n",
    "\n",
    "    return pd.DataFrame(metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_train_df(train_path, mask_path, images_to_ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Balance the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance between Pos and Neg classifications:\n",
    "len(df[df.Pneumothorax == True]), len(df[df.Pneumothorax == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, stratify=df.Pneumothorax, test_size=0.2, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img_height = 1024\n",
    "img_width = 1024\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, data_frame,\n",
    "                 batch_size=4,\n",
    "                 img_height=512,\n",
    "                 img_width=512,\n",
    "                 n_channels=3,\n",
    "                 augmentations=None,\n",
    "                 shuffle=True):\n",
    "        self.data_frame = data_frame\n",
    "        self.batch_size = batch_size\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.n_channels = n_channels\n",
    "        self.augment = augmentations\n",
    "        self.shuffle = shuffle\n",
    "        #self.indexes = np.arange(len(self.data_frame))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Batches per epoch'\n",
    "        return int(np.ceil(len(self.data_frame) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Update indexes when epoch ends'\n",
    "        self.indexes = np.arange(len(self.data_frame))\n",
    "        if self.shuffle == True:\n",
    "            #self.indexes = np.arange(len(self.data_frame))\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Get one batch'\n",
    "        batch_indexes = self.indexes[index*self.batch_size:min((index+1)*self.batch_size,len(self.data_frame))]\n",
    "        \n",
    "        batch_x, batch_y = self.data_generation(batch_indexes)\n",
    "        \n",
    "        if self.augment is None:\n",
    "            return np.array(batch_x)/255, np.array(batch_y)\n",
    "        else:\n",
    "            return np.stack([self.augment(image=x)['image'] for x in batch_x], axis=0), np.array(batch_y)\n",
    "        \n",
    "    def data_generation(self, batch_indexes):\n",
    "        if 'PixelArray' in self.data_frame.columns:\n",
    "            X = [cv2.resize(img, (self.img_height, self.img_width)) for img in self.data_frame.iloc[batch_indexes]['PixelArray']]\n",
    "        else:   \n",
    "            fps = self.data_frame.iloc[batch_indexes].FilePath\n",
    "            X = np.empty((len(fps), self.img_height, self.img_width))\n",
    "            for i, fp in enumerate(fps):\n",
    "                dcm = pydicom.dcmread(fp)\n",
    "                X[i] = cv2.resize(dcm.pixel_array, (self.img_height, self.img_width))\n",
    "        # Gray img to rgb\n",
    "        X = np.stack([X, X, X], axis=3)\n",
    "\n",
    "        Y = pd.concat([np.invert(self.data_frame.iloc[batch_indexes]['Pneumothorax'])+0,\n",
    "                      self.data_frame.iloc[batch_indexes]['Pneumothorax']+0],\n",
    "                      axis=1) \n",
    "        \n",
    "        return np.uint8(X), np.uint8(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
    "    RandomBrightness, RandomContrast, RandomGamma,OneOf,\n",
    "    ToFloat, ShiftScaleRotate, GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n",
    "    RGBShift, RandomBrightness, RandomContrast, GaussNoise, CenterCrop,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, RandomSizedCrop\n",
    ")\n",
    "\n",
    "AUGMENTATIONS_TRAIN = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    RandomGamma(p=0.5),\n",
    "    GridDistortion(p=0.5),\n",
    "    ShiftScaleRotate(\n",
    "        shift_limit=0.05,\n",
    "        scale_limit=0.1,\n",
    "        rotate_limit=10,\n",
    "        border_mode=cv2.BORDER_CONSTANT,\n",
    "        p=0.5\n",
    "    ),\n",
    "    RandomSizedCrop(min_max_height=(int(0.9*img_height), img_height), height=img_height, width=img_width, p=0.5),\n",
    "    CLAHE(),\n",
    "    ToFloat()\n",
    "], p=1)\n",
    "\n",
    "\n",
    "AUGMENTATIONS_TEST = Compose([\n",
    "    CLAHE(),\n",
    "    ToFloat()\n",
    "], p=1)\n",
    "\n",
    "# Add Gamma correction to test\n",
    "#from imgaug import augmenters as iaa\n",
    "#seq = iaa.Sequential([iaa.GammaContrast(1.2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixel_array(data, figsize=(10,10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(data, cmap=plt.cm.bone)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_pixel_array_overlay(data, label, figsize=(10,10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(data, cmap=plt.cm.bone)\n",
    "    plt.imshow(label, alpha=.3)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_grid(imgs, grid_width, grid_height):\n",
    "    max_imgs = grid_width*grid_height\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(grid_height, grid_width, figsize=(16, 12))\n",
    "    \n",
    "    for i, img in zip(range(max_imgs), imgs):\n",
    "        ax = axes[int(i/grid_width), i%grid_width]\n",
    "        ax.imshow(img, cmap='bone')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = DataGenerator(data_frame=train_df, batch_size=20, shuffle=False,\n",
    "                 img_height=img_height, img_width=img_width, n_channels=n_channels)\n",
    "imgs, masks = a.__getitem__(0)\n",
    "plot_grid(imgs[:,:,:,0], 5, 4)\n",
    "imgs[0].min(), imgs[0].max(), imgs.shape, imgs[0].shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Augmented\n",
    "b = DataGenerator(data_frame=train_df, batch_size=20, augmentations=AUGMENTATIONS_TRAIN, shuffle=False,\n",
    "                 img_height=img_height, img_width=img_width, n_channels=n_channels)\n",
    "imgs, masks = b.__getitem__(0)\n",
    "plot_grid(imgs[:,:,:,0], 5, 4)\n",
    "imgs[0].min(), imgs[0].max(), imgs.shape, imgs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader with Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune Pretrained Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet50(img_height=img_height, img_width=img_width, n_channels=n_channels):\n",
    "    inputs = Input(shape=(img_height, img_width, n_channels))\n",
    "    #conc = Concatenate()([inputs, inputs, inputs])\n",
    "    \n",
    "    base = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs, classes=2)\n",
    "    \n",
    "    for layer in base.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    outputs = GlobalAveragePooling2D()(base.output)\n",
    "    outputs = Dense(2, activation='softmax')(outputs)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "def get_resnext50(img_height=img_height, img_width=img_width, n_channels=n_channels):\n",
    "    inputs = Input(shape=(img_height, img_width, n_channels))\n",
    "    #conc = Concatenate()([inputs, inputs, inputs])\n",
    "    \n",
    "    base = ResNeXt50(include_top=False, weights='imagenet', input_tensor=inputs, classes=2,\n",
    "                    backend = keras.backend, layers = keras.layers, models = keras.models, utils = keras.utils)\n",
    "    \n",
    "    for layer in base.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    outputs = GlobalAveragePooling2D()(base.output)\n",
    "    outputs = Dense(2, activation='softmax')(outputs)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = get_resnet50()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.00001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint('./save/resnet50_best.h5', monitor='val_acc', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(data_frame=train_df,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width,\n",
    "                                   n_channels=n_channels,\n",
    "                                   batch_size=batch_size,\n",
    "                                   augmentations=AUGMENTATIONS_TRAIN)\n",
    "validation_generator = DataGenerator(data_frame=val_df,\n",
    "                                     img_height=img_height,\n",
    "                                     img_width=img_width,\n",
    "                                     n_channels=n_channels,\n",
    "                                     batch_size=batch_size,\n",
    "                                     augmentations=AUGMENTATIONS_TEST)\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                              validation_data=validation_generator,\n",
    "                              use_multiprocessing=False,\n",
    "                              epochs=epochs,\n",
    "                              verbose=1,\n",
    "                              callbacks=[early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./save/resnet50_final.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check on Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./save/resnet50_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = DataGenerator(data_frame=val_df,\n",
    "                                     img_height=img_height,\n",
    "                                     img_width=img_width,\n",
    "                                     n_channels=n_channels,\n",
    "                                     batch_size=batch_size,\n",
    "                                     augmentations=AUGMENTATIONS_TEST,\n",
    "                                     shuffle=False)\n",
    "preds_val = model.predict_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(np.equal(np.argmax(preds_val, axis=-1), np.array(val_df['Pneumothorax']+0))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
